{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HCCREN/whisper_voice/blob/main/WHISPER_TEST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCD1xgMnHPF0",
        "outputId": "81394d2d-99cd-49d4-b16b-46472ae7cd1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20231117.tar.gz (798 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/798.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/798.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m788.5/798.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.1.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper) (3.13.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801356 sha256=9999d2a0cff9e9a7c3e1fb0e9dfc2f0f4c7cfc011737dd55b455fe01ef8afed1\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/85/e1/9361b4cbea7dd4b7f6702fa4c3afc94877952eeb2b62f45f56\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-whisper-20231117 tiktoken-0.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -U openai-whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdmMd9CcI90Y",
        "outputId": "6ce9e1c2-3f2d-430a-8cc1-9b5daf672a60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install pydub\n",
        "!apt-get install ffmpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXBT2Eb5Zw9v"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozjH3ZyLJBud",
        "outputId": "08d8673c-cf7d-4ad8-f73b-0299cc440d53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SswwbRW7MyZQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pydub import AudioSegment\n",
        "import torch\n",
        "import whisper\n",
        "def m4a_to_wav(input_path, output_path):\n",
        "    audio = AudioSegment.from_file(input_path, \"m4a\")\n",
        "    audio.export(output_path, format=\"wav\")\n",
        "\n",
        "def split_audio(audio, interval_length_ms, output_folder):\n",
        "    # Calculate the number of segments needed\n",
        "    num_segments = len(audio) // interval_length_ms\n",
        "\n",
        "    # Split the audio\n",
        "    for i in range(num_segments):\n",
        "        start_time = i * interval_length_ms\n",
        "        end_time = (i + 1) * interval_length_ms\n",
        "        segment = audio[start_time:end_time]\n",
        "\n",
        "        # Export each segment\n",
        "        segment.export(f\"{output_folder}/segment_{i+1}.wav\", format=\"wav\")\n",
        "\n",
        "'''file_name = 'Sato1.m4a'\n",
        "input_file_path = '/content/drive/MyDrive/Colab Notebooks/sound_data/' + file_name  # Replace with your M4A file path\n",
        "output_file_path = '/content/drive/MyDrive/Colab Notebooks/sound_data/' + file_name.split('.')[0] + '.wav'  # Output WAV file path\n",
        "output_folder = '/content/drive/MyDrive/Colab Notebooks/sound_data/split_segments'  # Output folder for the split segments'''\n",
        "output_file_path = '/content/drive/MyDrive/Colab Notebooks/sound_data/news.wav'\n",
        "output_folder = '/content/drive/MyDrive/Colab Notebooks/sound_data/split_segments'\n",
        "# Convert M4A to WAV\n",
        "#m4a_to_wav(input_file_path, output_file_path)\n",
        "\n",
        "# Load the converted WAV file\n",
        "#audio = AudioSegment.from_wav(output_file_path)\n",
        "audio = AudioSegment.from_wav(output_file_path)\n",
        "# Interval length in milliseconds (e.g., 1000ms = 1s)\n",
        "interval_length_ms = 2640000\n",
        "\n",
        "# Split and save the audio segments\n",
        "split_audio(audio, interval_length_ms, output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vG7G5Rlub6mG",
        "outputId": "1e0e4e47-ac9a-4286-aa80-8bd116f22a4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 2.88G/2.88G [00:40<00:00, 75.9MiB/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Check if CUDA is available and set the device accordingly\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the whisper model\n",
        "model_size = \"large-v3\"  # or \"small\", \"medium\", \"large\" depending on the model size you want to load\n",
        "model = whisper.load_model(model_size).to(device)\n",
        "\n",
        "def transcribe_audio_files(folder_path, model, output_folder):\n",
        "    # List all the files in the folder\n",
        "    file_names = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
        "\n",
        "    for file_name in file_names:\n",
        "        if not file_name.endswith('.wav'):\n",
        "            continue  # Skip non-WAV files\n",
        "\n",
        "        # Load audio\n",
        "        audio_path = os.path.join(folder_path, file_name)\n",
        "        audio = whisper.load_audio(audio_path)\n",
        "\n",
        "        # Transcribe audio\n",
        "        text = model.transcribe(audio)\n",
        "\n",
        "        # Create and save text file\n",
        "        output_text_path = os.path.join(output_folder, f\"{file_name.split('.')[0]}_transcription.txt\")\n",
        "        with open(output_text_path, \"w\") as f:\n",
        "            f.write(text['text'])\n",
        "\n",
        "# Folder containing the audio segments\n",
        "input_folder = '/content/drive/MyDrive/Colab Notebooks/sound_data/split_segments'\n",
        "\n",
        "# Output folder for the transcriptions\n",
        "output_folder = '/content/drive/MyDrive/Colab Notebooks/sound_data/transcriptions'\n",
        "\n",
        "# Create the output folder if it doesn't exist\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# Transcribe and save text\n",
        "transcribe_audio_files(input_folder, model, output_folder)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMrsUjEZxDFUS4rz3vuzGCi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}